{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPx0AGmxBGEN"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "s0rtVwPyBlGJ",
    "outputId": "bb5ed64b-7497-4dc4-cd73-cb0d78c787ec"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsPS_I7xBGER",
    "outputId": "8da2ab16-27ed-4eb7-a1e1-5e793b0b5d0a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import copy\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from utility import *\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaQWPL9gBGEU"
   },
   "source": [
    "## Importing the Data\n",
    "1. TrainData - Original\n",
    "2. TestData - testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "4WKoO5LGBGEV"
   },
   "outputs": [],
   "source": [
    "original = pd.read_csv(\"./titanic/train.csv\")\n",
    "testDf = pd.read_csv(\"./titanic/test.csv\")\n",
    "test = copy.deepcopy(testDf)\n",
    "df = copy.deepcopy(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRlL2RLGBGEX"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "Types of Data:\n",
    "1. Continuous Data\n",
    "   1. Age\n",
    "   2. Fare\n",
    "2. Discrete Data\n",
    "   1. Survived\n",
    "   2. SibSp\n",
    "   3. Parch\n",
    "   4. PassengerId\n",
    "3. Categorical Data\n",
    "   1. Name\n",
    "   2. Pclass\n",
    "   3. Cabin\n",
    "   4. Embarked\n",
    "   5. Sex\n",
    "   6. Ticket\n",
    "\n",
    "![image.png](attachment:330bac07-c0a3-4fb3-a616-b64a7d6617eb.png)\n",
    "![image.png](attachment:35db6205-e342-429d-8b0e-53b2c388c43c.png)\n",
    "![image.png](attachment:c8eb2c98-3a62-4045-b958-318db589c437.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CooOmP2VBGEY"
   },
   "source": [
    "## Handling Missing Values\n",
    "1. Cabin\n",
    "2. Embarked\n",
    "3. Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2DqgPJZBGEZ",
    "outputId": "2e32c398-f664-4249-94a3-d956444c2141"
   },
   "outputs": [],
   "source": [
    "# Processing the Data to remove columns with >= 50% missing values and removing rows forming <= 5% missing values.\n",
    "# Dropping highly variable columns of Ticket, Name, PassengerId\n",
    "# Factorizing the remaining categorical data - LabelEncoder\n",
    "# This data point in Fare was missing, instead of making a whole model to impute the value, I replaced it with mode\n",
    "test.loc[test['Fare'].isna(), 'Fare'] = 7.812\n",
    "df = process_data(df)\n",
    "# Using Linear Regression to impute Ages.\n",
    "X_train, Y_train, X_test = create_splits(df.drop('Survived', axis = 1), \"Age\")\n",
    "# Better if we dont exclude Survived, but had to do for testData\n",
    "model = train_model(X_train, Y_train)\n",
    "df = impute_age(df, \"Age\", X_test, model)\n",
    "test = process_data(test)\n",
    "X_train, Y_train, X_test = create_splits(test, \"Age\")\n",
    "test = impute_age(test, \"Age\", X_test, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AecNtorgBGEb"
   },
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "F7EO21z1BGEb"
   },
   "outputs": [],
   "source": [
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiNaHHBwBGEc"
   },
   "source": [
    "## Outlier Detection\n",
    "1. Univariate Outliers\n",
    "2. Multivariate Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QIOcXmrLBGEc",
    "outputId": "74ea17d7-e219-433c-d60e-14e0f0a06c31"
   },
   "outputs": [],
   "source": [
    "# SibSp and Parch are discrete, so we can also cap them or remove them to certain level.. I am using Percentiles to cap according to my domain knowledge\n",
    "# The Data is highly skewed, therefore, IQR method will not work on it. As we cannot numericalize it. So we will go with Percentiles\n",
    "cols = ['SibSp', \"Parch\"]\n",
    "print('SibSp Value Count: ', df['SibSp'].value_counts())\n",
    "print('Parch Value Count: ', df['Parch'].value_counts())\n",
    "box_plot(df, cols)\n",
    "dist_plot(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp and Parch are discrete, so we can also cap them or remove them to certain level.. I am using Percentiles to cap according to my domain knowledge\n",
    "# The Data is highly skewed, therefore, IQR method will not work on it. As we cannot numericalize it. So we will go with Percentiles\n",
    "cols = ['SibSp', \"Parch\"]\n",
    "print('SibSp Value Count: ', df['SibSp'].value_counts())\n",
    "print('Parch Value Count: ', df['Parch'].value_counts())\n",
    "box_plot(df, cols)\n",
    "dist_plot(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjQQZcm4BGEd",
    "outputId": "ac161983-cfb8-40de-e61e-d344e6e9377a"
   },
   "outputs": [],
   "source": [
    "# According to me, the SibSp can be capped at 97.5 percentile\n",
    "upper_limit = np.percentile(df['SibSp'], 97.5)\n",
    "print(\"Upper Limit for SibSp: \", upper_limit)\n",
    "valuesToBeCapped = np.where(df[\"SibSp\"] > upper_limit)[0]\n",
    "df.loc[valuesToBeCapped, \"SibSp\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7N_QdiABGEd",
    "outputId": "1c14d0cb-9b14-47eb-c854-1cb0460005b1"
   },
   "outputs": [],
   "source": [
    "# Same thing for Parch\n",
    "# According to my knowledge, best answer was 4 after deeply analyzing the data and seeing the graphs\n",
    "upper_limit = np.percentile(df['Parch'], 98.5)\n",
    "print(\"Upper Limit for Parch: \", upper_limit)\n",
    "valuesToBeCapped = np.where(df[\"Parch\"]>upper_limit)[0]\n",
    "df.loc[valuesToBeCapped, \"Parch\"] = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8kC1eiFBGEe"
   },
   "source": [
    "### Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "g_OEf15XBGEe",
    "outputId": "7289676a-8eab-4ed6-d1d0-557040a842d4"
   },
   "outputs": [],
   "source": [
    "# I created another feature to get better idea, and applied log to normalize the data as the data for fare was also skewed, in order to perform IQR\n",
    "# I transformed into log data.\n",
    "df['Fare Per Person'] = df[\"Fare\"]/(df[\"SibSp\"]+df['Parch'] + 1)\n",
    "df = transform_log(df, 'Fare Per Person')\n",
    "col = ['Log_transformed_Fare Per Person']\n",
    "box_plot(df, col)\n",
    "dist_plot(df, col)\n",
    "col = 'Log_transformed_Fare Per Person'\n",
    "lower_limit, upper_limit = calc_iqr(df, col)\n",
    "data = copy.deepcopy(df)\n",
    "data = data[(data[col] < upper_limit)]\n",
    "# 4 is the most suitable lowerLimit after analyzing the data\n",
    "data = data[data['Fare'] >= 4]\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "Tcf-rT5RBGEe",
    "outputId": "bd701395-b41f-46dd-c8e1-344ec032c90f"
   },
   "outputs": [],
   "source": [
    "col = ['Age']\n",
    "box_plot(data, col)\n",
    "stanD = data['Age'].std()\n",
    "meanD = data['Age'].mean()\n",
    "cappedValue = meanD + 3*stanD\n",
    "lower_limit, upper_limit = calc_iqr(data, \"Age\")\n",
    "print(\"Upper Limit for Age:\", upper_limit)\n",
    "valuesToBeCapped = np.where(data[col] > 65)[0]\n",
    "data.loc[valuesToBeCapped, \"Age\"] = cappedValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt4CH6BJBGEf"
   },
   "source": [
    "## We are done with univariate, now its time for multivariate Outliers.\n",
    "I have used Isolation Forest, but have to preprocess as it only accepts numerical data. I have not used other techniques such as Mahalanobis Distance because it requires normal distribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "ew8zeuxVBGEg",
    "outputId": "57dbcd76-6fe6-4abd-81a2-69ca13fc9314"
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding for Gender and Cities\n",
    "new_data = copy.deepcopy(data)\n",
    "new_data\n",
    "new_data = pd.get_dummies(new_data, columns= [\"Embarked\", \"Sex\"])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "6g7buZcXBGEg",
    "outputId": "92ad2de3-5d28-4838-bbc0-8353b43cb10f"
   },
   "outputs": [],
   "source": [
    "# Converting into Normal Data. And removing Log data as I dont want them to be part\n",
    "new_data = new_data.replace({True: 1, False: 0})\n",
    "new_data['Pclass'] = new_data['Pclass'].replace({3: 1, 1:3})\n",
    "cols = [col for col in new_data.columns if \"Log\" in col]\n",
    "print(cols)\n",
    "new_data.drop(cols, axis = 1, inplace = True)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "FPRDX3McBGEh",
    "outputId": "26e71e73-763f-414d-e1e4-7d57f32487db"
   },
   "outputs": [],
   "source": [
    "# Multivariate Outliers\n",
    "anomaly_data = copy.deepcopy(new_data)\n",
    "model = IsolationForest(n_estimators= 100, contamination=0.015, random_state=42)\n",
    "model.fit(anomaly_data)\n",
    "new_data['Anomaly Score'] = model.decision_function(anomaly_data)\n",
    "new_data[\"Anomaly\"] = model.predict(anomaly_data)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "id": "W7yeR_MtBGEi",
    "outputId": "3c498b26-8a60-4c10-9798-7826ada192d9"
   },
   "outputs": [],
   "source": [
    "# Removing those outliers as they are multivariate and it will be difficult to adjust them again and reversing the one hot encoding\n",
    "# as it is not needed now.\n",
    "new_data = new_data[new_data[\"Anomaly\"] != -1]\n",
    "new_data.reset_index(drop = True, inplace= True)\n",
    "cols = [\"Sex\", \"Embarked\"]\n",
    "reverse_one_hot(new_data, cols)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "QiZFR3C_BGEi",
    "outputId": "a9799336-649a-4737-90d3-1b646ea6bf09"
   },
   "outputs": [],
   "source": [
    "# Removing them\n",
    "cols = ['Anomaly Score', \"Anomaly\"]\n",
    "new_data.drop(cols, inplace = True, axis = 1)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INQf0JQGBGEj"
   },
   "source": [
    "## Feature Analysis Feature Selection\n",
    "I am going to use Partial, Complete Corelation and Mutual Information to find the most effective features for my dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-f_l-RfJBGEj",
    "outputId": "ffd8aeef-13e9-4731-9ceb-ec6ae3ba20b1"
   },
   "outputs": [],
   "source": [
    "new_data = transform_log(new_data, \"Fare Per Person\")\n",
    "analyzed_data = copy.deepcopy(new_data)\n",
    "pcCorr = compute_partial_relation(analyzed_data, \"Survived\")\n",
    "print(analyzed_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6CMgX7DBGEk",
    "outputId": "ac06722f-b588-4dd8-f14a-43af30a8f8aa"
   },
   "outputs": [],
   "source": [
    "# Using MI - Discretizing as MI works good on discrete data\n",
    "cols = [\"Fare\", \"Log_transformed_Fare Per Person\", \"Age\"]\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "analyzed_data[cols] = discretizer.fit_transform(analyzed_data[cols])\n",
    "mi_scores = mutual_info_classif(analyzed_data.drop(\"Survived\", axis = 1), analyzed_data[\"Survived\"], random_state=42)\n",
    "mi_df = pd.DataFrame({'Feature': analyzed_data.drop(\"Survived\", axis = 1).columns, 'MI Score': mi_scores})\n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "xHiYDNNWBGEk",
    "outputId": "e34ed25e-40ce-4556-d91c-1bb2a2618731"
   },
   "outputs": [],
   "source": [
    "corrFull = new_data.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corrFull, annot=True, cmap=\"coolwarm\", center=0, linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arJthluZBGEk"
   },
   "source": [
    "### Notes\n",
    "I have removed Fare Per Person and SibSp for better results and added Log Fare Per Person along with Total People that accounts for SibSp.\n",
    "I am going to remove SibSp, and Fare per person as SibSp p value was against it and i think log will perform better as its results were more statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HF9nFmnBGEk",
    "outputId": "058760aa-d983-4306-913a-5136d271b889"
   },
   "outputs": [],
   "source": [
    "new_data['Total People'] = new_data[\"Parch\"] + new_data[\"SibSp\"] + 1\n",
    "test['Total People'] = test['Parch'] + test['SibSp'] + 1\n",
    "test['Fare Per Person'] = test[\"Fare\"]/(test[\"SibSp\"]+test['Parch'] + 1)\n",
    "test = transform_log(test, 'Fare Per Person')\n",
    "new_data.drop(inplace=True, axis = 0, columns = ['Fare Per Person', 'SibSp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppH2lglcBGEl"
   },
   "source": [
    "## Final Touches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "2_AbCKu5BGEl",
    "outputId": "84b8f542-67cf-4f8f-ca8d-7fca72b40a91",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding\n",
    "cols = [\"Sex\", \"Embarked\"]\n",
    "encoded_data = pd.get_dummies(data = new_data, columns = cols)\n",
    "encoded_data = encoded_data.replace({True: 1, False: 0})\n",
    "encoded_data = encoded_data[sorted(encoded_data)]\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCfgSbmABGEm",
    "outputId": "90013018-15d8-4335-ce03-ef0b579151a1"
   },
   "outputs": [],
   "source": [
    "X_train.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true,
    "id": "Bvbtfm0cBGEm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = encoded_data.drop(\"Survived\", axis = 1)\n",
    "Y_train = encoded_data['Survived']\n",
    "model = RandomForestClassifier(n_jobs = -1,  max_depth = 4, n_estimators=50)\n",
    "model.fit(X_train, Y_train)\n",
    "y_train = model.predict(X_train)\n",
    "cv_scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM3I-jKaYyYn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "v2fianbVXgeM",
    "outputId": "0156c267-fde2-4120-986c-de68a0118ff0"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93xePkoUBGEn",
    "outputId": "bb762e2b-da74-4b0e-c82c-68a960c3f3b9"
   },
   "outputs": [],
   "source": [
    "print(\"Cross-Validation Accuracy:\", np.mean(cv_scores))\n",
    "print(\"NICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "NOCaEnisBGEn",
    "outputId": "4fa4e7a3-4561-494b-cb80-18572c892c61"
   },
   "outputs": [],
   "source": [
    "test = test.replace({-np.inf:0})\n",
    "test.drop(inplace=True, axis = 0, columns = ['Fare Per Person', 'SibSp'])\n",
    "test = test.replace({True: 1, False: 0})\n",
    "test['Pclass'] = test['Pclass'].replace({3: 1, 1:3})\n",
    "cols = [\"Sex\", \"Embarked\"]\n",
    "encoded_data_test = pd.get_dummies(data = test, columns = cols)\n",
    "X_test = encoded_data_test[sorted(encoded_data_test)]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "GOImZIfIBGEn"
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "X4I_PgGlBGEn"
   },
   "outputs": [],
   "source": [
    "np.savetxt('arrayFinal1.csv', y_test, delimiter=',', fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "GmfhoXWIBGEo"
   },
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "FD77i75KBGEq"
   },
   "outputs": [],
   "source": [
    "testResult = pd.read_csv('./titanic/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "koMKiPnYBGEq"
   },
   "outputs": [],
   "source": [
    "testResult['Survived'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "WMy-jwYUBGEr",
    "outputId": "e536cf17-4256-44a0-f120-02723ae19cfc"
   },
   "outputs": [],
   "source": [
    "testResult.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "t81hORVwBGEr"
   },
   "outputs": [],
   "source": [
    "testResult.to_csv('Submission_file_nameRandom2.csv', index=False , header = 1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szRuLUItBGEr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
